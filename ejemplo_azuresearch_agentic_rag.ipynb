{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic RAG con Autogen utilizando Azure AI Services\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import asyncio\n",
    "from typing import List, Dict\n",
    "\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_core import CancellationToken\n",
    "from autogen_agentchat.messages import TextMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from autogen_ext.models.azure import AzureAIChatCompletionClient\n",
    "\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import SearchIndex, SimpleField, SearchFieldDataType, SearchableField\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creando el cliente\n",
    "\n",
    "Inicializamos el cliente Azure AI Chat Completion, después lo usaremos para interactuar con el servicio de Azure OpenAI para generar respuestas a las queries del usuario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AzureAIChatCompletionClient(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    endpoint=\"https://models.inference.ai.azure.com\",\n",
    "    credential=AzureKeyCredential(os.getenv(\"GITHUB_TOKEN\")),\n",
    "    model_info={\n",
    "        \"json_output\": True,\n",
    "        \"function_calling\": True,\n",
    "        \"vision\": True,\n",
    "        \"family\": \"unknown\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Database \n",
    "\n",
    "Inicializamos Azure AI Search con memoria persistente y varios ejemplos de documentos. Azure AI Search sera utilizado para guardar y obtener los documentos que después serán el contexto para generar respuestas congruentes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<azure.search.documents._generated.models._models_py3.IndexingResult at 0x2299c1b8200>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x2299c1b9d90>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x2299c1b9bb0>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x2299c1b9d00>,\n",
       " <azure.search.documents._generated.models._models_py3.IndexingResult at 0x2299c1b9c70>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inicializamos AzureAI\n",
    "search_service_endpoint = os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\")\n",
    "search_api_key = os.getenv(\"AZURE_SEARCH_API_KEY\")\n",
    "index_name = \"travel-documents\"\n",
    "\n",
    "search_client = SearchClient(\n",
    "    endpoint=search_service_endpoint,\n",
    "    index_name=index_name,\n",
    "    credential=AzureKeyCredential(search_api_key)\n",
    ")\n",
    "\n",
    "index_client = SearchIndexClient(\n",
    "    endpoint=search_service_endpoint,\n",
    "    credential=AzureKeyCredential(search_api_key)\n",
    ")\n",
    "\n",
    "# Index schema\n",
    "fields = [\n",
    "    SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True),\n",
    "    SearchableField(name=\"content\", type=SearchFieldDataType.String)\n",
    "]\n",
    "\n",
    "index = SearchIndex(name=index_name, fields=fields)\n",
    "\n",
    "# Creamos el index\n",
    "index_client.create_index(index)\n",
    "\n",
    "\n",
    "documents = [\n",
    "    {\"id\": \"1\", \"content\": \"Paraguay Travel ofrece paquetes de vacaciones de lujo a destinos exóticos en todo el mundo.\"},\n",
    "    {\"id\": \"2\", \"content\": \"Nuestros servicios de viaje premium incluyen planificación personalizada de itinerarios y asistencia de conserjería 24/7.\"},\n",
    "    {\"id\": \"3\", \"content\": \"El seguro de viaje de Paraguay cubre emergencias médicas, cancelaciones de viaje y pérdida de equipaje.\"},\n",
    "    {\"id\": \"4\", \"content\": \"Los destinos populares incluyen Cerro Paraguari, los Alpes Suizos y safaris africanos.\"},\n",
    "    {\"id\": \"5\", \"content\": \"Paraguay Travel ofrece acceso exclusivo a hoteles boutique y tours privados con guía.\"}\n",
    "]\n",
    "\n",
    "\n",
    "# Añadimos documentos al index\n",
    "search_client.upload_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_retrieval_context(query: str) -> str:\n",
    "    results = search_client.search(query)\n",
    "    context_strings = []\n",
    "    for result in results:\n",
    "        context_strings.append(f\"Documento: {result['content']}\")\n",
    "    return \"\\n\\n\".join(context_strings) if context_strings else \"No hay resultados\"\n",
    "\n",
    "def get_weather_data(location: str) -> str:\n",
    "    \"\"\"\n",
    "    Acá simulamos la obtención de datos del clima, podemos conectarnos también a una API del clima para esto\n",
    "    \"\"\"\n",
    "    weather_database = {\n",
    "        \"new york\": {\"temperature\": 72, \"condition\": \"Parcialmente nublado\", \"humidity\": 65, \"wind\": \"10 mph\"},\n",
    "        \"london\": {\"temperature\": 60, \"condition\": \"Lluvioso\", \"humidity\": 80, \"wind\": \"15 mph\"},\n",
    "        \"tokyo\": {\"temperature\": 75, \"condition\": \"Soleado\", \"humidity\": 50, \"wind\": \"5 mph\"},\n",
    "        \"sydney\": {\"temperature\": 80, \"condition\": \"Despejado\", \"humidity\": 45, \"wind\": \"12 mph\"},\n",
    "        \"paris\": {\"temperature\": 68, \"condition\": \"Nublado\", \"humidity\": 70, \"wind\": \"8 mph\"},\n",
    "    }\n",
    "    \n",
    "    # Normalizar la cadena de ubicación\n",
    "    location_key = location.lower()\n",
    "    \n",
    "    # Verificar si tenemos datos para esta ubicación\n",
    "    if location_key in weather_database:\n",
    "        data = weather_database[location_key]\n",
    "        return f\"Clima para {location.title()}:\\n\" \\\n",
    "               f\"Temperatura: {data['temperature']}°F\\n\" \\\n",
    "               f\"Condición: {data['condition']}\\n\" \\\n",
    "               f\"Humedad: {data['humidity']}%\\n\" \\\n",
    "               f\"Viento: {data['wind']}\"\n",
    "    else:\n",
    "        return f\"No hay datos meteorológicos disponibles para {location}.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración del agente\n",
    "\n",
    "Configuramos el agente de retrieval y el asistente. El retrieval va a ser un agente especializado en encontrar información relevante utilizando búsqueda semántica, mientras que el asistente generará respuestas detalladas basadas en la información del retrieval. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = AssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    model_client=client,\n",
    "    system_message=(\n",
    "        \"Eres un asistente de IA útil que proporciona respuestas utilizando ÚNICAMENTE el contexto proporcionado. \"\n",
    "        \"NO incluyas información externa. Basa tu respuesta completamente en el contexto dado a continuación.\"\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAGEvaluator Class\n",
    "\n",
    "La clase RAGEvaluation va a evaluar las respuestas basadas en metricas como el largor, las fuentes de información citadas, el tiempo de respuesta y la relevancia del contexto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGEvaluator:\n",
    "    def __init__(self):\n",
    "        self.responses: List[Dict] = []\n",
    "\n",
    "    def evaluate_response(self, query: str, response: str, context: List[Dict]) -> Dict:\n",
    "        # Las métricas: \n",
    "        start_time = time.time()\n",
    "        metrics = {\n",
    "            'response_length': len(response),\n",
    "            'source_citations': sum(1 for doc in context if doc[\"content\"] in response),\n",
    "            'evaluation_time': time.time() - start_time,\n",
    "            'context_relevance': self._calculate_relevance(query, context)\n",
    "        }\n",
    "        self.responses.append({\n",
    "            'query': query,\n",
    "            'response': response,\n",
    "            'metrics': metrics\n",
    "        })\n",
    "        return metrics\n",
    "\n",
    "    def _calculate_relevance(self, query: str, context: List[Dict]) -> float:\n",
    "        # Generamos un score de las métricas\n",
    "        return sum(1 for c in context if query.lower() in c[\"content\"].lower()) / len(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesamiento del Query con RAG\n",
    "\n",
    "Definimos una función ask_rag que va a mandar el query (del backend) al asistente, procesa la respuesta y luego la evalúa. Esta función maneja la interacción con el asistente y luego usa el evaluador para medir la calidad de las respuestas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def ask_unified_rag(query: str, evaluator: RAGEvaluator, location: str = None):\n",
    "    \"\"\"\n",
    "    Una función RAG unificada que combina tanto la recuperación de documentos\n",
    "    como datos meteorológicos según la consulta y un parámetro opcional de ubicación.\n",
    "    \n",
    "    Args:\n",
    "        query: La pregunta del usuario\n",
    "        evaluator: El evaluador RAG para medir la calidad de la respuesta\n",
    "        location: Ubicación opcional para consultas sobre el clima\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Obtener contexto de ambas fuentes\n",
    "        retrieval_context = get_retrieval_context(query)\n",
    "        \n",
    "        # Si se proporciona ubicación, agregar datos del clima\n",
    "        weather_context = \"\"\n",
    "        if location:\n",
    "            weather_context = get_weather_data(location)\n",
    "            weather_intro = f\"\\nInformación del clima para {location}:\\n\"\n",
    "        else:\n",
    "            weather_intro = \"\"\n",
    "        \n",
    "        # Ampliar la consulta con ambos contextos si están disponibles\n",
    "        augmented_query = (\n",
    "            f\"Contexto recuperado:\\n{retrieval_context}\\n\\n\"\n",
    "            f\"{weather_intro}{weather_context}\\n\\n\"\n",
    "            f\"Consulta del usuario: {query}\\n\\n\"\n",
    "            \"Basándote ÚNICAMENTE en el contexto anterior, por favor proporciona la respuesta.\"\n",
    "        )\n",
    "\n",
    "        # Enviar la consulta ampliada como mensaje del usuario\n",
    "        start_time = time.time()\n",
    "        response = await assistant.on_messages(\n",
    "            [TextMessage(content=augmented_query, source=\"user\")],\n",
    "            cancellation_token=CancellationToken(),\n",
    "        )\n",
    "        processing_time = time.time() - start_time\n",
    "\n",
    "        # Crear contexto combinado para la evaluación\n",
    "        combined_context = documents.copy()  # Comenzar con los documentos de viaje\n",
    "        \n",
    "        # Agregar el clima como documento si existe\n",
    "        if location and weather_context:\n",
    "            combined_context.append({\"id\": f\"weather-{location}\", \"content\": weather_context})\n",
    "        \n",
    "        # Evaluar la respuesta\n",
    "        metrics = evaluator.evaluate_response(\n",
    "            query=query,\n",
    "            response=response.chat_message.content,\n",
    "            context=combined_context\n",
    "        )\n",
    "        \n",
    "        result = {\n",
    "            'response': response.chat_message.content,\n",
    "            'processing_time': processing_time,\n",
    "            'metrics': metrics,\n",
    "        }\n",
    "        \n",
    "        # Agregar ubicación al resultado si se proporciona\n",
    "        if location:\n",
    "            result['location'] = location\n",
    "            \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error al procesar la consulta unificada: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    evaluator = RAGEvaluator()\n",
    "    \n",
    "    # Definir consultas del usuario similares al ejemplo de Semantic Kernel\n",
    "    user_inputs = [\n",
    "        # Consultas solo de viaje\n",
    "        {\"query\": \"¿Puedes explicar la cobertura del seguro de viaje de Paraguay Travel?\"},\n",
    "        \n",
    "        # Consultas solo de clima \n",
    "        {\"query\": \"¿Cuál es la condición climática actual en London?\", \"location\": \"london\"},\n",
    "        \n",
    "        # Consultas combinadas\n",
    "        {\"query\": \"¿Cuál es un destino frío ofrecido por Paraguay Travel y cuál es su temperatura?\", \"location\": \"london\"},\n",
    "    ]\n",
    "    \n",
    "    print(\"Procesando consultas:\")\n",
    "    for query_data in user_inputs:\n",
    "        query = query_data[\"query\"]\n",
    "        location = query_data.get(\"location\")\n",
    "        \n",
    "        if location:\n",
    "            print(f\"\\nProcesando consulta para {location}: {query}\")\n",
    "        else:\n",
    "            print(f\"\\nProcesando consulta: {query}\")\n",
    "        \n",
    "        # Obtener el contexto RAG para mostrarlo (similar al ejemplo de Semantic Kernel)\n",
    "        retrieval_context = get_retrieval_context(query)\n",
    "        weather_context = get_weather_data(location) if location else \"\"\n",
    "        \n",
    "        # Mostrar el contexto RAG para mayor transparencia\n",
    "        print(\"\\n--- Contexto RAG ---\")\n",
    "        print(retrieval_context)\n",
    "        if weather_context:\n",
    "            print(f\"\\n--- Contexto del clima para {location} ---\")\n",
    "            print(weather_context)\n",
    "        print(\"-------------------\\n\")\n",
    "            \n",
    "        result = await ask_unified_rag(query, evaluator, location)\n",
    "        if result:\n",
    "            print(\"Respuesta:\", result['response'])\n",
    "            print(\"\\nMétricas:\", result['metrics'])\n",
    "        print(\"\\n\" + \"=\"*60 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Para hacer correr el script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    if asyncio.get_event_loop().is_running():\n",
    "        await main()\n",
    "    else:\n",
    "        asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
